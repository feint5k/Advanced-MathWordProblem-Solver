{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Initial Data Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihfXrF0zE-5H",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivCIhm4zyRyI",
        "colab_type": "code",
        "outputId": "69d1f301-789c-4c09-8059-bb9a72a53c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import json, csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries Imported\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries Imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvxifhcnAx6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getText(doc):\n",
        "  doc =  str(doc)\n",
        "  doc = doc.lower().strip()\n",
        "  doc = re.sub('\\n', ' ', doc)\n",
        "  doc = re.sub(r'\\s+', ' ', doc)\n",
        "  m = re.search(r'<meta property=\\\"og:title\\\" content=\\\"(.*?)\\\"/>',doc)\n",
        "  m1 = re.search(r'<meta property=\\\"og:description\\\" content=\\\"(.*?)\\\"/>',doc)\n",
        "\n",
        "  if m != None and m1!= None:\n",
        "    text = str(m.group(1)) + ' ' + str(m1.group(1))\n",
        "  else:\n",
        "    text  = \"No match\"\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EXob7wzFNcz",
        "colab_type": "text"
      },
      "source": [
        "Preparing Datasets\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgcH2x0NY6Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_json('eval_cleaned.json')\n",
        "\n",
        "for i, row in data.iterrows():\n",
        "  if re.match(r\"^\\\"/><meta property=\\\"\", row['original_text']) == None:\n",
        "    text = getText(row['original_text'])\n",
        "    if text != \"No match\":\n",
        "      data.at[i,'text'] = text\n",
        "  items = row['equations'].split(\"\\r\\n\")\n",
        "  data.at[i,'nequ'] = len(items)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00sJsbIPc18g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.loc[data['nequ'] == 2]\n",
        "\n",
        "data[[\"unknowns\",\"equations\"]] = data[\"equations\"].str.split(\"\\r\\n\", 1, expand = True)\n",
        "\n",
        "data[\"unknowns\"] = data[\"unknowns\"].str[6:]\n",
        "\n",
        "data[\"equations\"] = data[\"equations\"].str[5:]\n",
        "\n",
        "data = data[[\"text\",\"ans\",\"equations\",\"unknowns\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQCi_lZnJKwi",
        "colab_type": "code",
        "outputId": "e9d64326-09e4-4a8b-f5ea-95e3209ad1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "data = data.dropna(subset=[\"equations\"])\n",
        "\n",
        "data.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6625 entries, 19 to 14722\n",
            "Data columns (total 4 columns):\n",
            "text         6625 non-null object\n",
            "ans          6625 non-null object\n",
            "equations    6625 non-null object\n",
            "unknowns     6625 non-null object\n",
            "dtypes: object(4)\n",
            "memory usage: 258.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_j4sf8UFRAO",
        "colab_type": "text"
      },
      "source": [
        "Data Cleaning\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdhRZ_R1FTCI",
        "colab_type": "code",
        "outputId": "136792b6-a0ff-4cc7-b1d7-5ae7cf3f4d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy\n",
        "\n",
        "# import string\n",
        "\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "spacy_stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "stopset = nltk_stopwords.union(spacy_stopwords)\n",
        "\n",
        "stopset.difference_update([\"a\",\"more\",\"less\",\"than\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\",\"eleven\",\"twelve\",\"fifteen\",\"twenty\",\"forty\",\"sixty\",\"fifty\",\"hundred\",\"once\",\"first\",\"second\",\"third\"])\n",
        "\n",
        "punctuation = \"!\\\"#$&',;?@\\_`{|}~\"\n",
        "\n",
        "def getText(doc):\n",
        "  doc =  str(doc)\n",
        "  doc = doc.lower().strip()\n",
        "  doc = re.sub('\\n', ' ', doc)\n",
        "  doc = re.sub(r'\\s+', ' ', doc)\n",
        "  m = re.search(r'<meta property=\\\"og:title\\\" content=\\\"(.*?)\\\"/>',doc)\n",
        "  m1 = re.search(r'<meta property=\\\"og:description\\\" content=\\\"(.*?)\\\"/>',doc)\n",
        "\n",
        "  if m != None and m1!= None:\n",
        "    text = str(m.group(1)) + ' ' + str(m1.group(1))\n",
        "  else:\n",
        "    text  = \"No match\"\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def cleanData(doc):\n",
        "  doc = str(doc)\n",
        "  doc = doc.lower().strip()\n",
        "  doc = re.sub('\\n', ' ', doc)\n",
        "  doc = re.sub(r'\\s+', ' ', doc)\n",
        "  pattern = '\"/><meta '\n",
        "  lookup = re.search(pattern, doc)\n",
        "  if lookup != None:\n",
        "    index = doc.find(lookup.group(0))\n",
        "    doc = doc[:index]\n",
        "  doc = doc.replace('yahoo answers','')\n",
        "  regex1 = r\"[A-Za-z\\s*]+[\\.:%-][A-Za-z\\s*]*\"\n",
        "  regex2 = r\"[0-9]+\\.[0-9]+\"\n",
        "  regex3 = r\"[0-9\\s*]{1,}[\\=\\*+-][0-9\\s*]{1,}\"\n",
        "  match1 = re.search(regex1, doc)\n",
        "  match2 = re.search(regex2, doc)\n",
        "  match3 = re.search(regex3, doc)\n",
        "  # tokens = nltk.WordPunctTokenizer().tokenize(doc)      \n",
        "  tokens = doc.split()\n",
        "  # clean = [token.lower().strip() for token in tokens if token not in stopset]\n",
        "  clean = [token for token in tokens if token not in stopset]\n",
        "  clean = [token.replace('.','') for token in clean if not re.search(r\"[0-9]+\\.[0-9]+\", token)]\n",
        "  clean = \" \".join(clean)\n",
        "  # clean = [''.join(c for c in s if c not in punctuation) for s in clean]\n",
        "  final = ''.join(c for c in clean if c not in punctuation)\n",
        "  # final_tokens = final.split()\n",
        "  # clean2 = [token.replace('.','') for token in final_tokens if not re.search(r\"[0-9]+\\.[0-9]+\", token)]\n",
        "  # final = \" \".join(clean2)\n",
        "  # final = \" \".join(clean)\n",
        "  try:\n",
        "      final = remove_whitespace(final.encode('latin1').decode('utf-8','replace').encode('ascii','ignore'))\n",
        "  except:\n",
        "      final = remove_whitespace(final)\n",
        "  final=final.strip()\n",
        "  return final\n",
        "\n",
        "def remove_whitespace(x):\n",
        "  \"\"\"\n",
        "  Helper function to remove any blank space from a string\n",
        "  x: a string\n",
        "  \"\"\"\n",
        "  try:\n",
        "      # Remove spaces inside of the string\n",
        "      x = \" \".join(x.split())\n",
        "  except:\n",
        "      pass\n",
        "  return x\n",
        "  \n",
        "print(\"Functions Defined!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Functions Defined!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlwEQ2xHH1s9",
        "colab_type": "code",
        "outputId": "a2677378-e528-4e80-c590-8f30df44dc10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "data['cleaned_text'] = data.apply(lambda x: cleanData(str(x['text'])), axis=1)\n",
        "\n",
        "data['cleaned_text'] = data['cleaned_text'].str.decode(\"utf-8\")\n",
        "\n",
        "data = data.dropna(subset=[\"cleaned_text\"])\n",
        "\n",
        "data.info()\n",
        "\n",
        "# data.to_csv(\"trainData_univariable.csv\", index = False)\n",
        "# data.to_csv(\"trainData_univariable.txt\", index = False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6487 entries, 19 to 14722\n",
            "Data columns (total 5 columns):\n",
            "text            6487 non-null object\n",
            "ans             6487 non-null object\n",
            "equations       6487 non-null object\n",
            "unknowns        6487 non-null object\n",
            "cleaned_text    6487 non-null object\n",
            "dtypes: object(5)\n",
            "memory usage: 304.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpkqG1aVO2Z5",
        "colab_type": "text"
      },
      "source": [
        "Data Modelling (Archieve)\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvbmU8_8XLBX",
        "colab_type": "code",
        "outputId": "c7fd0123-7402-4bdd-e6ff-7d2fcdb47475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('new_cleaned_data.csv')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "trainData, testData = train_test_split(data, test_size = 0.2)\n",
        "\n",
        "trainData.rename(columns={'cleaned text': 'cleaned_text'}, inplace=True)\n",
        "\n",
        "testData.rename(columns={'cleaned text': 'cleaned_text'}, inplace=True)\n",
        "\n",
        "trainData = trainData.reset_index(drop=True)\n",
        "\n",
        "testData = testData.reset_index(drop=True)\n",
        "\n",
        "print(trainData.info())\n",
        "\n",
        "print(testData.info())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame